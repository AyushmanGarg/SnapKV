
\textbf{Prompt Length (}\(L_{\text{prompt}}\)\textbf{):} The total length of the user-provided input.
\textbf{Prefix Length (}\(L_{\text{prefix}}\)\textbf{):} The length of the input preceding the observation window. It is part of the prompt and does not include the observation window.
\textbf{Observation Window (}\(L_{\text{obs}}\)\textbf{):} The last segment of the prompt. This window is crucial for analyzing the influence of different contexts on attention patterns.
These definitions are interconnected as follows:
\begin{equation}\label{eq: length}
L_{\text{prompt}} = L_{\text{prefix}} + L_{\text{obs}}
\end{equation}
\textbf{Voting:} The process of calculating attention weights for each query within the observation window across all heads, aggregating these weights to highlight the prefix positions that are considered most significant.

For a single batch of sequence, formally:
\begin{align}
\mathbf{C} &= \sum_{i=0}^{L_{\text{obs}}} \mathbf{W}_{\text{obs}}[:, i, :] \\
I &= \text{Top}_k(\mathbf{C}, k)
\end{align}

where \(\text{Top}_k(\mathbf{T}, k)\) selects the indices of the top \(k\) values in tensor \(\mathbf{T}\) per head, \(k\) is defined as \(\left\lfloor\textit{p} \times L_{\text{prefix}}\right\rfloor\). The tensor \(\mathbf{W}_{\text{obs}}\in \mathbb{R}^{N \times L_{\text{obs}} \times L_{\text{prefix}}}\) represents the subset of the prompt softmax-normalized attention features over $N$ heads.


\textbf{Hit Rate:} The hit rate, \(H\), quantifies the effectiveness of the voting mechanism by measuring the ratio of attention features identified as significant by the voting process that are also essential in the generation outcome, calculated as:

\begin{align}
\mathbf{M}_{\text{vote\_obs}} &= \text{zeros\_like} (\mathbf{A}_{\text{cur}}) \\
\mathbf{M}_{\text{vote\_obs}}[I] &= 1 \\
\mathbf{M}_{\text{threshold\_cur}} &= \mathbf{1}(\mathbf{A}_{\text{cur}} > \theta) \\
\mathbf{O} &= \mathbf{M}_{\text{threshold\_cur}} \land \mathbf{M}_{\text{vote\_obs}} \label{eq:stepa}\\
H &= \frac{\sum \mathbf{O}}{\sum \mathbf{M}_{\text{threshold\_cur}}} \label{eq:stepb}
\end{align}


\(\mathbf{A}_{\text{cur}}\in \mathbb{R}^{N \times L_{\text{prefix}}}\) represents the attention features between the current generated query and prefix keys. The threshold operation filters \(\mathbf{A}_{\text{cur}}\) to retain only values exceeding \(\theta\), indicating significant attention activations. The overlap \(\mathbf{O}\) between these significant activations and the mask \(\mathbf{M}\) quantifies the alignment of the current attention with previously identified significant features. The hit rate \(H\) is then computed as the ratio of the sum of overlap \(\mathbf{O}\) to the sum of significant activations \(\mathbf{A}_{\text{threshold}}\), providing a metric for the efficacy of the attention mechanism in recognizing and emphasizing important attention features within the context. We can use \(\mathcal{H}(\mathbf{M}_{\text{threshold\_cur}}, \mathbf{M}_{\text{vote\_obs}})\) denote combination of eq.~\ref{eq:stepa} and eq.~\ref{eq:stepb}. We use $p = 0.05$ (top 5\% location per head) and $\theta = 0.05$ (note it is a large value due to the softmax function over a long sequence) for the observation experiments. The model we probe is \texttt{Mistral-7B-Instruct-v0.2}.






\subsection{Observations in Multi-Turn Conversations}
\label{sec: Multi-Turn Conversations}

\begin{figure}[ht]
    \centering
    % \includegraphics[scale = 0.2]{figures/hit_rate_1k.png}
    % \includegraphics[scale = 0.2]{figures/hit_rate_1.5k.png}
    % \includegraphics[scale = 0.2]{figures/hit_rate_2k.png}
    % \includegraphics[scale = 0.2]{figures/hit_rate_2.5k.png}
    \includegraphics[width=0.7\textwidth]{figures/hit_rate_3k.pdf}
    \caption{
    The layer-wise average hit rate of important positions utilized along token generation with an average input length exceeding 3k.
    }
    \label{fig: hit_rate}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/question_front.pdf}
    \includegraphics[width=0.7\textwidth]{figures/question_back.pdf}
    \caption{The layer-wise average hit rate of important positions utilized by prompts with questions at the beginning and the end.}
    \label{fig: question_pos}
\end{figure}

This study examines if the positions of features identified as crucial in the observation window maintain their significance in the subsequent token generation. The analysis utilizes samples from Ultrachat~\cite{ding2023enhancing}, a multi-turns, high-quality instruction dataset consisting of 1.4 million dialogues. We further filter the sequences with response length greater than 512 and prompt length greater than 3k. In the experiment, we split the generated tokens into 4 context windows, each spanning 128 tokens, to compute the averaged hit rates of these windows versus the observation window with size 32. According to the findings presented in Fig.\ref{fig: hit_rate}, important keys in prefixes obtained from voting in observation windows exhibit remarkable consistency throughout the generation process, as evidenced by high hit rates. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/different_answer_pairs.pdf}
    \caption{The layer-wise overlap of important positions utilized by different question-answer pairs in the same dataset.
    }
    \label{fig: qa_pairs}
\end{figure}
\subsection{Observations in Long Document QA}
To further validate this finding, we also observe on multiple long documents QA datasets including QMSum \cite{zhong2021qmsum}, a query-based multi-domain meeting summarization; Openreview \cite{an2023eval}, a collection of papers from \texttt{openreview.net}; SPACE \cite{angelidis2021extractive}, an extractive opinion summarization in quantized transformer spaces.


\subsubsection{Effectiveness of Instruction Positions}
\label{sec: Do Question Positions Matter?}
Our investigation also extends to the significance of instruction positioning on the interpretability of LLMs and their selection of important features. We calculate the average hit rate for the responses using the same observation window size of 32 as in the previous experiment. Our results shown in Fig. \ref{fig: question_pos} indicate that across all three datasets, the hit rates are consistently high regardless of whether instructions are positioned before or after extensive supplementary contexts. This consistency suggests that the patterns identified by observation windows are independent of the question's positions.



\subsubsection{Effectiveness of Various Instructions for One Document}
\label{sec: Do Different Questions Matter for One Document?}

Furthermore, we investigate whether instructions will affect the selection of important features even if the provided context is the same. Our experiment utilizes different instructions on the same document and selects the important features based on the observation window that consists of both the instructions and their corresponding responses. Then we calculate the hit rates between important features selected by different instruction-response pairs within the same document by using \(\mathcal{H}(\text{M}_{\text{vote\_A}}, \text{M}_{\text{vote\_B}})\). By varying the instructions, we observe that different instructions prioritize different prefix keys, as indicated by the descending trend in hit rates shown in Fig. \ref{fig: qa_pairs}. 
Our findings reveal an interesting aspect of KV cache management in LLMs: the important attention features change with different instructions. This variability challenges the effectiveness of static compression methods that depend on constant weighted importance or fixed policies~\cite{liu2024scissorhands,zhang2024h2o,ge2023model}. Thus, the complex relationship between context and related KV cache emphasizes the need for context-aware compression strategies and highlights the limitations of current methods that ignore this dynamic.
\section{SnapKV}
\label{sec: 4}
\subsection{Basic Method}

In the attention mechanism, keys and values are tensors containing information from the previous context. The linear growth in prompts will lead to exponential time complexity for generation due to the Query-Key matrix multiplication. \kv addresses this by keeping prompt KV cache counts constant during generation, significantly reducing serving times for long-context LLMs.
